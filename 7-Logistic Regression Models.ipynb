{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58695798-afbc-409b-96bf-6b3ab870fbfa",
   "metadata": {},
   "source": [
    "* İlk classification algoritması olan logistic regression ile başlayalım.\n",
    "* Classıfıcatıon algoritması 0'ları 1'leri öğrenir sonra farklı bir fotoğrafra 0 ve 1'i tahmin eder\n",
    "* Kedi köpek sınıflandırması örnek olarak verilebilir.\n",
    "* Deep learningin temelidir.\n",
    "![resim](7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76526997-d6d9-4153-bad8-0f62a6f38ce0",
   "metadata": {},
   "source": [
    "* Her bir kare bir pixeldir.\n",
    "<br>\n",
    "\n",
    "\n",
    "![resim](8.jpg)\n",
    "![resim](9.jpg)\n",
    "\n",
    "* np array 1 2 3 4 5 .......\n",
    "* .........................\n",
    "* ..................... 4096 diye dizilir\n",
    "* boyutu 4096*1 lik numpy array oluyor\n",
    "* 64*64 luk resmi numpy arraye çevirdik daha sonra da bunu lojistil regresyon yapmak için kullanıcaz\n",
    "* bir resmi train etmek için hazırlık yapıyoruz yani "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d84e7-3c8f-4a81-90df-afe1a3e9295b",
   "metadata": {},
   "source": [
    "* Computation graph  matematiksel ifadeleri görselleştirmek için bir yöntemdir.\n",
    "* Lojistik regresyonu anlatırken bu computation graph üzerinden gidicez.\n",
    "\n",
    "![resim](10.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58c1dda-6bd2-4544-89b3-61f64d2d81da",
   "metadata": {},
   "source": [
    "- 4096 tane feature train edicez bu resim için modelimize\n",
    "- Parameters are weight and bias.\n",
    "- Weights: coefficients of each pixels\n",
    "- Bias: intercept\n",
    "- z = (w.t)x + b => z equals to (transpose of weights times input x) + bias\n",
    "- In an other saying => z = b + px1w1 + px2w2 + ... + px4096*w4096\n",
    "- y_head = sigmoid(z)\n",
    "- Sigmoid function makes z between zero and one so that is probability. You can see sigmoid function in computation graph.\n",
    "- Why we use sigmoid function?\n",
    "- It gives probabilistic result\n",
    "- It is derivative so we can use it in gradient descent algorithm (we will see as soon.)\n",
    "- Lets make example:\n",
    "- Lets say we find z = 4 and put z into sigmoid function. The result(y_head) is almost 0.9. It means that our classification - result is 1 with 90% probability.\n",
    "- Now lets start with from beginning and examine each component of computation graph more detailed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264f958-8bee-4ff3-b821-0c5d720a95fe",
   "metadata": {},
   "source": [
    " - y= a0 + a1*x de a0 bias a1 de katsayı\n",
    " - sigmoid function : aktivasyon fonksiyonu olarak geçer ve 0 ile 1 arasına değeri sokar. Ayrıca türevi alınabilen bir değer biraz da o yüzden kullanırız. w1 w2 ... değerleri bu sayede güncellenebilir.\n",
    " - Türevi alınamayan bir şey kullanırsak weight ve bias değerlerini güncelleyemeyiz.\n",
    " - 0,5 diye threshold koyuyorum mesela 0.5'ten büyükler 1 küçükler 0 gibi\n",
    " - z = -2 olsun mesela tabloya göre 0.1 geliyor sonuç . 0.5ten küçük old için 0 derim\n",
    " - Soru 1 : Yanlış tajminde ne yaoılır ?\n",
    " - Soru 2 : weight ve bias değerleri ilk olarak neye göre belirlenir? ---> Rastgele atanır yanlış tahmin edince terse doğru gidip türev alınıp bias ve weight değerleri güncellenir. Weight ve bias güncelleye güncelleye tüm resimleri eğitiyoruz. TTüm resimlere göre eğitilmiş modelimizi buluyoruz. En sondaki weight ve bias bizim modelin katsayıları olur.\n",
    " - Soru 3 : tüm fotoğrafları nasıl modelimize sokucaz ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a142e-3474-4e99-a76d-38dcda76b793",
   "metadata": {},
   "source": [
    "- weight ve bias değerlerini ilk başta nasıl seçtiğimie bakalım\n",
    "- \n",
    "## Initializing parameters\n",
    "- As you know input is our images that has 4096 pixels(each image in x_train).\n",
    "- Each pixels have own weights.\n",
    "- The first step is multiplying each pixels with their own weights.\n",
    "- The question is that what is the initial value of weights?\n",
    "- There are some techniques that I will explain at artificial neural network but for this time initial weights are 0.01.\n",
    "- Okey, weights are 0.01 but what is the weight array shape? As you understand from computation graph of logistic regression, - it is (4096,1)\n",
    "- Also initial bias is 0.\n",
    "- Lets write some code. In order to use at coming topics like artificial neural network (ANN), I make definition(method).\n",
    "- Parametrelerin ilk değerini bul\n",
    "- Seçme tekniklerini deep learning de öğrenicez . Burda ınıtıal değer olarak 0.01 seçebiliriz.\n",
    "- weıght 0 seçilirse model öğrenemez onun için 0 seçilmemeli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03488110-ecc9-4ee0-ac99-d94572063cf6",
   "metadata": {},
   "source": [
    "## Forward Propagation¶\n",
    "- The all steps from pixels to cost is called forward propagation\n",
    "- z = (w.T)x + b => in this equation we know x that is pixel array, we know w (weights) and b (bias) so the rest is calculation. (T is transpose)\n",
    "- Then we put z into sigmoid function that returns y_head(probability). When your mind is confused go and look at computation graph. Also equation of sigmoid function is in computation graph.\n",
    "- Then we calculate loss(error) function.\n",
    "- Cost function is summation of all loss(error).\n",
    "- Lets start with z and the write sigmoid definition(method) that takes z as input parameter and returns y_head(probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5e18b1-b221-46d8-9cc5-5cbb70886a10",
   "metadata": {},
   "source": [
    "- Forward z ile başlar.\n",
    "- ![resim](11.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5e88a-e4e8-42ab-b7df-bd63ecb7376a",
   "metadata": {},
   "source": [
    "- resim 1 tahmin 1 ise loss 0 ,  resim 0 tahmin 0 ise loss 0 ,  ama resim 1 tahmin 0 ise loss yüksek\n",
    "- loss function ile forward kısmına baktık.\n",
    "- loss function her bir resim için hatanın bulunmasıdır. Cost function da bu lossların toplamıdır. Eğer ki cost function yüksek ise kötü modele sahibiz demektir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a5c13-3116-4b30-9f38-c2a41ce046bf",
   "metadata": {},
   "source": [
    "- bu resimleri for döngüsü ile tek tek sokmak zor olur, zaman alır. Numpy array ile yapılabilir. Tüm matrisi ayrı ayrı forward yapmaktansa tekte yapıyorum ve 348 loss değeri elde ediyorum. Sonra da bu 348 değeri topluyorum ve bir tane cost değeri elde ediyorum. Amaç cost fonksiyonunu azaltmak bu da bias ve weight güncellemesi ile olabilir. Gradient descent metodu ile weight ve bias değerlerini optimize edicez ve en uygun değeri bulucaz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9381e702-19ca-410b-bb3c-a00e917fd1be",
   "metadata": {},
   "source": [
    "## Optimization Algorithm with Gradient Descent¶\n",
    "- Well, now we know what is our cost that is error.\n",
    "- Therefore, we need to decrease cost because as we know if cost is high it means that we make wrong prediction.\n",
    "- Lets think first step, every thing starts with initializing weights and bias. Therefore cost is dependent with them.\n",
    "- In order to decrease cost, we need to update weights and bias.\n",
    "- In other words, our model needs to learn the parameters weights and bias that minimize cost function. This technique is called gradient descent.\n",
    "- Lets make an example:\n",
    "\n",
    "- We have w = 5 and bias = 0 (so ignore bias for now). Then we make forward propagation and our cost function is 1.5.\n",
    "It looks like this. (red lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f68bb-bc2c-4e1f-83db-1a1baba67310",
   "metadata": {},
   "source": [
    "![resim](12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6629f-7f7a-432f-8402-39062d6abbee",
   "metadata": {},
   "source": [
    "- As you can see from graph, we are not at minimum point of cost function. Therefore we need to go through minimum cost. Okey, lets update weight. ( the symbol := is updating)\n",
    "- w := w - step. The question is what is this step? Step is slope1. Okey, it looks remarkable. In order to find minimum point, we can use slope1. Then lets say slope1 = 3 and update our weight. w := w - slope1 => w = 2.\n",
    "- Now, our weight w is 2. As you remember, we need to find cost function with forward propagation again.\n",
    "- Lets say according to forward propagation with w = 2, cost function is 0.4. Hmm, we are at right way because our cost function is decrease. We have new value for cost function that is cost = 0.4. Is that enough? Actually I do not know lets try one more step.\n",
    "- Slope2 = 0.7 and w = 2. Lets update weight w : = w - step(slope2) => w = 1.3 that is new weight. So lets find new cost.\n",
    "- Make one more forward propagation with w = 1.3 and our cost = 0.3. Okey, our cost even decreased, it looks like fine but is it enough or do we need to make one more step? The answer is again I do not know, lets try.\n",
    "- Slope3 = 0.01 and w = 1.3. Updating weight w := w - step(slope3) => w = 1.29 ~ 1.3. So weight does not change because we find minimum point of cost function.\n",
    "- Everything looks like good but how we find slope? If you remember from high school or university, in order to find slope of function(cost function) at given point(at given weight) we take derivative of function at given point. Also you can ask that okey well we find slope but how it knows where it go. You can say that it can go more higher cost values instead of going minimum point. The asnwer is that slope(derivative) gives both step and direction of step. Therefore do not worry :)\n",
    "- Update equation is this. It says that there is a cost function(takes weight and bias). Take derivative of cost function according to weight and bias. Then multiply it with α learning rate. Then update weight. (In order to explain I ignore bias but these all steps will be applied for bias)\n",
    "![resim](13.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1532aa16-8407-4601-a904-31a44a8a6aa4",
   "metadata": {},
   "source": [
    "- Now, I am sure you are asking what is learning rate that I mentioned never. It is very simple term that determines learning rate. Hovewer there is tradeoff between learning fast and never learning. For example you are at Paris(current cost) and want to go Madrid(minimum cost). If your speed(learning rate) is small, you can go Madrid very slowly and it takes too long time. On ther other hand, if your speed(learning rate) is big, you can go very fast but maybe you make crash and never go to Madrid. Therefore, we need to choose wisely our speed(learning rate).\n",
    "- Learning rate is also called hyperparameter that need to be chosen and tuned. I will explain it more detailed in artificial neural network with other hyperparameters. For now just say learning rate is 1 for our previous example.\n",
    "- I think now you understand the logic behind forward propagation(from weights and bias to cost) and backward propagation(from cost to weights and bias to update them). Also you learn gradient descent. Before implementing the code you need to learn one more thing that is how we take derivative of cost function according to weights and bias. It is not related with python or coding. It is pure mathematic. There are two option first one is to google how to take derivative of log loss function and second one is even to google what is derivative of log loss function :) I choose second one because I cannot explain math without talking :)\n",
    "![resim](14.jpg)\n",
    "resim 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28862da-690b-479a-9cdb-35bf5cd7392f",
   "metadata": {},
   "source": [
    "- Cost functiondan başlar geriye doğru gider. weight ve bias değerleri güncellenir.\n",
    "- slope yani eğim bizim stepimiz bu stepe göre w güncellendikten sonra yeniden forward yapılır\n",
    "- 1.3 değerinde cost bulduk buranın min olduğunu bilmiyoruz devam edicez. slope 3 nerdeyse 0 değişim yok . Değişim yokksa min noktaya ulaşmışımdır.\n",
    "- Türevini alıp 0'a eşitleme ile bu yola gidebiliriz.\n",
    "- Bir noktaya göre türev = eğim\n",
    "- min noktada 0 çıkar\n",
    "- w: w-a.... ---> step yani cost fonksiyonun w ye göre türevini al\n",
    "- bias için de aynı şeyler geçerlidir.\n",
    "- learning rate : öğrenme hızı olarak ifade edilebilir.\n",
    "- learning rate yavaş ise çok yavaş gideriz. Hızlı ise hiç gidemeyebiliiriz. Kaza yapabiliriz mantığı. Ex : lr =1 iken 5-3= 2 diye gider. lr : 3 iken 5-9 dan -4 olur mın noktadan uzaklaşırızı \n",
    "- hyperparameter : önce seçicez sonra deneye deneye ayarlıcaz\n",
    "- resim 14 : türevin matematiksel karşılığıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2beb547f-01c2-4ea2-9ccf-7adbd12335e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "857e5852-7c07-4cab-a47b-b555a4b2ab9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% read csv\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afa46ab5-df30-429f-ab79-62a446eb347c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# M : Kötü huylu tümör B : İyi huylu tümor\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "791fa3be-8f29-4508-935a-c8cf1958ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"Unnamed: 32\", \"id\"] ,axis=1 , inplace=True) # gereksiz bilgi içerenleri ya da bilgi içermeyenleri cıkardık"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3c40977-2b91-4cd2-a9f2-e1c0aa2515d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m ve b değerleri kategorik ya da int olması lazım. 0 ve 1 'e atıcaz bu değerleri.\n",
    "data.diagnosis = [ 1 if each ==\"M\" else 0 for each in data.diagnosis ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2e2971f-baa5-4d30-aa5a-ed99438c1d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0          1        17.99         10.38          122.80     1001.0   \n",
       "1          1        20.57         17.77          132.90     1326.0   \n",
       "2          1        19.69         21.25          130.00     1203.0   \n",
       "3          1        11.42         20.38           77.58      386.1   \n",
       "4          1        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a09e595d-c451-4669-b7d3-ce2767ffd9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   diagnosis                569 non-null    int64  \n",
      " 1   radius_mean              569 non-null    float64\n",
      " 2   texture_mean             569 non-null    float64\n",
      " 3   perimeter_mean           569 non-null    float64\n",
      " 4   area_mean                569 non-null    float64\n",
      " 5   smoothness_mean          569 non-null    float64\n",
      " 6   compactness_mean         569 non-null    float64\n",
      " 7   concavity_mean           569 non-null    float64\n",
      " 8   concave points_mean      569 non-null    float64\n",
      " 9   symmetry_mean            569 non-null    float64\n",
      " 10  fractal_dimension_mean   569 non-null    float64\n",
      " 11  radius_se                569 non-null    float64\n",
      " 12  texture_se               569 non-null    float64\n",
      " 13  perimeter_se             569 non-null    float64\n",
      " 14  area_se                  569 non-null    float64\n",
      " 15  smoothness_se            569 non-null    float64\n",
      " 16  compactness_se           569 non-null    float64\n",
      " 17  concavity_se             569 non-null    float64\n",
      " 18  concave points_se        569 non-null    float64\n",
      " 19  symmetry_se              569 non-null    float64\n",
      " 20  fractal_dimension_se     569 non-null    float64\n",
      " 21  radius_worst             569 non-null    float64\n",
      " 22  texture_worst            569 non-null    float64\n",
      " 23  perimeter_worst          569 non-null    float64\n",
      " 24  area_worst               569 non-null    float64\n",
      " 25  smoothness_worst         569 non-null    float64\n",
      " 26  compactness_worst        569 non-null    float64\n",
      " 27  concavity_worst          569 non-null    float64\n",
      " 28  concave points_worst     569 non-null    float64\n",
      " 29  symmetry_worst           569 non-null    float64\n",
      " 30  fractal_dimension_worst  569 non-null    float64\n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 137.9 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee4ef377-f53f-4cec-812c-03cf6389fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 tane clasımız oldu iyi huylu ve kötü huyl diye \n",
    "# diagnosis harici hepsi x ekseni featurlar\n",
    "\n",
    "y= data.diagnosis.values\n",
    "x_data = data.drop([\"diagnosis\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fc44f82-ac26-42c8-9cfb-6ee2ea69753a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ccca160-ffe5-4c1a-89cb-9733534310d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aec05c4d-4a26-49e6-a462-3bcb57c37e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52103744, 0.0226581 , 0.54598853, ..., 0.91202749, 0.59846245,\n",
       "        0.41886396],\n",
       "       [0.64314449, 0.27257355, 0.61578329, ..., 0.63917526, 0.23358959,\n",
       "        0.22287813],\n",
       "       [0.60149557, 0.3902604 , 0.59574321, ..., 0.83505155, 0.40370589,\n",
       "        0.21343303],\n",
       "       ...,\n",
       "       [0.45525108, 0.62123774, 0.44578813, ..., 0.48728522, 0.12872068,\n",
       "        0.1519087 ],\n",
       "       [0.64456434, 0.66351031, 0.66553797, ..., 0.91065292, 0.49714173,\n",
       "        0.45231536],\n",
       "       [0.03686876, 0.50152181, 0.02853984, ..., 0.        , 0.25744136,\n",
       "        0.10068215]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% normalization\n",
    "# normalize etmek gerekir çünkü bir fature değeri 2500 diğeri 0.008166 .Büyük olan küçüğü ortadan kaldırabilir.\n",
    "# normalize edip en sonunda np arraye çevirmek için . valıues dedik\n",
    "x = (x_data - x_data.min()) / (x_data.max() - x_data.min())\n",
    "x = x.values  # En sonunda numpy array'e çeviriyoruz\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc64d55-9434-4313-b7ad-788224bc73c9",
   "metadata": {},
   "source": [
    "![resim](15.jpg)\n",
    "\n",
    "%80 train %20 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "328bb9b5-7f01-4de3-9033-4e4d3021efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2 ,random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0a50f3c-ec58-4d6f-b5db-bd381385a26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09692839, 0.25769361, 0.10365559, ..., 0.60137457, 0.52493594,\n",
       "        0.40968123],\n",
       "       [0.66775522, 0.57017247, 0.68350494, ..., 0.9975945 , 0.49931007,\n",
       "        0.48117539],\n",
       "       [0.10374367, 0.14034494, 0.10648884, ..., 0.174811  , 0.33845851,\n",
       "        0.19585465],\n",
       "       ...,\n",
       "       [0.34592267, 0.2404464 , 0.32140142, ..., 0.11453608, 0.17602996,\n",
       "        0.04040404],\n",
       "       [0.33125089, 0.33513696, 0.32706793, ..., 0.62783505, 0.31815494,\n",
       "        0.33097206],\n",
       "       [0.24605992, 0.36557322, 0.23101375, ..., 0.25536082, 0.22255076,\n",
       "        0.090122  ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75b03af5-2dbc-4043-9d61-1ac7aeab1561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25978513, 0.30064254, 0.25775689, ..., 0.34879725, 0.2856298 ,\n",
       "        0.2129083 ],\n",
       "       [0.56599934, 0.39228948, 0.55151683, ..., 0.61477663, 0.19436231,\n",
       "        0.07116621],\n",
       "       [0.4012968 , 0.33040243, 0.40017967, ..., 0.52027491, 0.25073921,\n",
       "        0.1649613 ],\n",
       "       ...,\n",
       "       [0.21482323, 0.17653027, 0.207864  , ..., 0.33017182, 0.2166371 ,\n",
       "        0.1511872 ],\n",
       "       [0.34260968, 0.61345959, 0.33694976, ..., 0.28243986, 0.06406466,\n",
       "        0.15033451],\n",
       "       [0.65071702, 0.72404464, 0.63513233, ..., 0.57010309, 0.25684999,\n",
       "        0.17801391]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c835bcff-9df9-4d71-89fd-49cd57329a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13cf5d1d-b93e-4aa7-9d0c-c17508466690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 30)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape # feature ve resim sayısı ters olmuş (30,455) olmalı\n",
    "#x_train2 = x_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "947ed1b3-70f2-496a-8223-8d8523ca3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c774aa9-64a4-450b-a5a1-8ccf5666d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6bd3262a-8bb6-410a-849d-849699d5f7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (30, 455)\n",
      "x_test:  (30, 114)\n",
      "y_train:  (455,)\n",
      "y_test:  (114,)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train: \",x_train.shape)\n",
    "print(\"x_test: \",x_test.shape)\n",
    "print(\"y_train: \",y_train.shape)\n",
    "print(\"y_test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d5fe83a-9b02-4ff0-bcd1-ae9d6f3c69e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% parameter initialize and sigmoid function\n",
    "#dimension = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5bde6111-bb63-47ce-8f32-a28ffe0f7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = np.full(5,1),0.01) # 5 e 1 lik 0.01 oluşturur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a90bc61-c182-4cff-9e0e-67c0814f75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights_and_bias(dimension):\n",
    "    w = np.full((dimension,1),0.01) # weightlerin ilk değerini atadık\n",
    "    b = 0.0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "964377cd-7b82-429f-a164-e180e3846b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w,b = initialize_weights_and_bias(30)\n",
    "# print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d588a5e4-bd80-4e2e-8736-6a1152c91f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7213bb2e-63c6-4716-bbbe-b9a9631643ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8939e81d-812b-4850-9429-0f8729881874",
   "metadata": {},
   "source": [
    "![resim](16.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a621a6ee-fe7f-4e43-b9c7-5dbc2e8dd58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    y_head = 1/(1+ np.exp(-z))\n",
    "    return y_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a7e5c4c-abd4-4bb1-9ab2-6b847b02a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sigmoid(0)) # sigmoid 6 = 0.99 sigmoid -100 = 3.7200e-44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7ce017b-775b-454b-a210-090a9a8dd070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_propagation(w,b,x_train,y_train):\n",
    "    # forward propagation\n",
    "    z = np.dot(w.T,x_train) + b\n",
    "    y_head = sigmoid(z)\n",
    "    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n",
    "    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n",
    "    \n",
    "    # backward propagation\n",
    "    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n",
    "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n",
    "    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n",
    "    \n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4dcc3da9-40d7-44c7-8ffc-18a9f9a836c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matris çarpımını sağlayabilmek adına (1,30) * (30,455) = (1,455) işlemi sağlanmalıdır.\n",
    "# iki matrisi çarpmak demek pixel ile weight çarpımı\n",
    "# türevleri bulduktan sonra  w = w-slope1 kısımlarını yapıcaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e4b1d3f-144f-42ca-97de-2daa534c3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Updating(learning) parameters  # weight ve bias update edilecek \n",
    "# number of iteration : kaç kez forward ve backward yapılcağının sayısı\n",
    "def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n",
    "    cost_list = [] # ilerde analiz için\n",
    "    cost_list2 = [] # 2000 iterasyon varrsa 200 tane cost değeri yazdırır. \n",
    "    index = []\n",
    "    \n",
    "    # updating(learning) parameters is number_of_iterarion times\n",
    "    for i in range(number_of_iterarion):\n",
    "        # make forward and backward propagation and find cost and gradients\n",
    "        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n",
    "        cost_list.append(cost) # cost tutma sebebi number of iteration sayısını bilmek # tüm costları depolamak için\n",
    "        # lets update  En optimize weight ve bias değerini buluyoruz burda. Train ediyoruz yani.\n",
    "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
    "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "        if i % 10 == 0:\n",
    "            cost_list2.append(cost) # her 10 adımda bir costları depolamak için \n",
    "            index.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "     # cost grafiğine göre number of iteration ayarlıcez       \n",
    "    # we update(learn) parameters weights and bias\n",
    "    parameters = {\"weight\": w,\"bias\": b} # depoluyoruz ki test etmek için kullanıcaz  test*w +b gibi\n",
    "    plt.plot(index,cost_list2)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterarion\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters, gradients, cost_list # modelimi türevlerimi ve cost listi return ediyorum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b12291d8-5b7c-4a9f-9068-d443cafb6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction ---------- model hazırsa forward yap 114 sample 30 feature\n",
    "def predict(w,b,x_test):\n",
    "    # x_test is a input for forward propagation\n",
    "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1])) # prediction matrisi oluştuyuroyrum x_test.shape[1]---> 114 1e 114 luk yani test boyutlarında. 0 lardan matirisi oluşturduk aşağıda test için doldurucaz\n",
    "    # (114,) sıkıntı çıkarmaz yanında 1 kabul edildiği için ( yatay vektöre ya da dikey vektör fark etmez tek satır ya da tek sütün old için) ancak 2,114 --> 114,2 şeklinde T alınmalı \n",
    "    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n",
    "    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8f9711a1-5996-456f-85b8-745a3dc2b447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.692977\n",
      "Cost after iteration 10: 0.499667\n",
      "Cost after iteration 20: 0.406616\n",
      "Cost after iteration 30: 0.351936\n",
      "Cost after iteration 40: 0.315762\n",
      "Cost after iteration 50: 0.289862\n",
      "Cost after iteration 60: 0.270257\n",
      "Cost after iteration 70: 0.254795\n",
      "Cost after iteration 80: 0.242214\n",
      "Cost after iteration 90: 0.231722\n",
      "Cost after iteration 100: 0.222796\n",
      "Cost after iteration 110: 0.215080\n",
      "Cost after iteration 120: 0.208317\n",
      "Cost after iteration 130: 0.202324\n",
      "Cost after iteration 140: 0.196961\n",
      "Cost after iteration 150: 0.192121\n",
      "Cost after iteration 160: 0.187722\n",
      "Cost after iteration 170: 0.183698\n",
      "Cost after iteration 180: 0.179997\n",
      "Cost after iteration 190: 0.176577\n",
      "Cost after iteration 200: 0.173402\n",
      "Cost after iteration 210: 0.170443\n",
      "Cost after iteration 220: 0.167676\n",
      "Cost after iteration 230: 0.165080\n",
      "Cost after iteration 240: 0.162638\n",
      "Cost after iteration 250: 0.160334\n",
      "Cost after iteration 260: 0.158155\n",
      "Cost after iteration 270: 0.156091\n",
      "Cost after iteration 280: 0.154131\n",
      "Cost after iteration 290: 0.152266\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 96.49122807017544 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% logistic_regression\n",
    "# y train w ve b'yi update edebilmesi için gereklidir.\n",
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n",
    "    # initialize\n",
    "    dimension =  x_train.shape[0]  # that is 30\n",
    "    w,b = initialize_weights_and_bias(dimension)\n",
    "    # do not change learning rate\n",
    "    # forward backward çağırmak yerine updata çağırıyorum ki o da zaten forward backward çağırıyor.\n",
    "    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n",
    "    \n",
    "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "\n",
    "    # Print test Errors\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "    \n",
    "logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 300)  # bunu for döngüsü içinde döndürürsek en yüksek accuracy değerini  bulabiliriz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d0e23-1ecf-43e2-b5d1-08e591a527a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "16a36329-3bda-431c-8e91-c94277b409d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "#%% sklearn with LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train.T,y_train.T)\n",
    "print(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581913a-83b0-4103-bd6c-01fd412e31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train: \",x_train.shape)\n",
    "print(\"x_test: \",x_test.shape)\n",
    "print(\"y_train: \",y_train.shape)\n",
    "print(\"y_test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c2c8d7-749d-44a8-9b6e-4f890e73a10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae402aa-a85d-41de-8627-eb803b02cb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2148e5d-6c31-4a96-a275-1b2284a0d010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba91ae7-3fa7-4ce6-b6ac-d87ed93e3c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37713af8-c3fe-45ed-bcd6-f0216c014ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3caf74-721a-4c37-9777-fdcbb350f504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c845451-bd7d-40a0-94ca-eff19530e225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be043a7-2748-4617-b242-4407de1d5fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf87014-120a-4b8c-8b44-a8859bc1bf25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5da9f-265d-44f2-ae8a-0126e2180302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6350988-cdf5-4796-b491-d263c3eb4f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18c834-e61f-4d56-a3c3-a97e9189643d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258099c4-1227-46ab-a6da-6a16ccab3071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
